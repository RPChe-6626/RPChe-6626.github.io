<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RPChe_&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/086eee41c000f36a571b2465dfa38e8e</icon>
  
  <link href="https://rpche-6626.github.io/atom.xml" rel="self"/>
  
  <link href="https://rpche-6626.github.io/"/>
  <updated>2025-04-22T16:12:12.083Z</updated>
  <id>https://rpche-6626.github.io/</id>
  
  <author>
    <name>RPChe_</name>
    <email>3420302325@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>典型集与渐进均分性</title>
    <link href="https://rpche-6626.github.io/2025/04/22/IT/aep/"/>
    <id>https://rpche-6626.github.io/2025/04/22/IT/aep/</id>
    <published>2025-04-21T16:00:00.000Z</published>
    <updated>2025-04-22T16:12:12.083Z</updated>
    
    
    <summary type="html">&lt;p&gt;Typical Set 和 Asymptotic Equipartition Property
应该是很深刻的性质。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="信息论" scheme="https://rpche-6626.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rpche-6626.github.io/2025/04/22/IT/defs/"/>
    <id>https://rpche-6626.github.io/2025/04/22/IT/defs/</id>
    <published>2025-04-21T16:00:00.000Z</published>
    <updated>2025-04-22T16:08:40.007Z</updated>
    
    
    <summary type="html">&lt;p&gt;大多数人可能会认为信息熵抽象而不明确，这的确很难避免；另一些人会认为信息熵仅是玻尔兹曼熵的推广、或者信息熵度量了信息量，这很大程度上是不对的。严格的说，信息熵的定义一方面依赖于概率论，其度量了随机变量的不确定度；另一方面则依赖于信息的编码理论。下面我们要从两个方面导出信息熵。并且如若不特别说明，我们只考虑离散的情况。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="信息论" scheme="https://rpche-6626.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>马尔可夫链与法诺不等式</title>
    <link href="https://rpche-6626.github.io/2025/04/22/IT/mark/"/>
    <id>https://rpche-6626.github.io/2025/04/22/IT/mark/</id>
    <published>2025-04-21T16:00:00.000Z</published>
    <updated>2025-04-22T16:08:45.829Z</updated>
    
    
    <summary type="html">&lt;p&gt;下面我们将简要介绍 Markov chain 与 Fano&#39;s Inequality 。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="信息论" scheme="https://rpche-6626.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>熵的性质</title>
    <link href="https://rpche-6626.github.io/2025/04/22/IT/prop/"/>
    <id>https://rpche-6626.github.io/2025/04/22/IT/prop/</id>
    <published>2025-04-21T16:00:00.000Z</published>
    <updated>2025-04-22T16:08:51.267Z</updated>
    
    
    <summary type="html">&lt;p&gt;熵有一些显然的性质，例如其是凹函数，这里就不赘述了。下面我们将定义一些其它种类的熵，并介绍其性质。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="信息论" scheme="https://rpche-6626.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>熵率</title>
    <link href="https://rpche-6626.github.io/2025/04/22/IT/rate/"/>
    <id>https://rpche-6626.github.io/2025/04/22/IT/rate/</id>
    <published>2025-04-21T16:00:00.000Z</published>
    <updated>2025-04-22T16:08:57.373Z</updated>
    
    
    <summary type="html">&lt;p&gt;熵率是要探讨信源编码的长期平均速率的极限。现在我们先要复习一些概率论基础，我本来是不想在这边写这些的，但是我没单独写概率论，所以还是提一下。以后的内容均在一定程度上依赖于基础实分析。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="信息论" scheme="https://rpche-6626.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>基础信息论</title>
    <link href="https://rpche-6626.github.io/2025/04/21/IT/%E5%9F%BA%E7%A1%80%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    <id>https://rpche-6626.github.io/2025/04/21/IT/%E5%9F%BA%E7%A1%80%E4%BF%A1%E6%81%AF%E8%AE%BA/</id>
    <published>2025-04-20T16:00:00.000Z</published>
    <updated>2025-04-22T16:12:36.491Z</updated>
    
    
    <summary type="html">&lt;p&gt;本篇是我复习信息论的时候写的。本来想把标题取作“写给小学生看的信息论”之类的，但感觉有点怪，所以就叫做《基础信息论》了。本文的重点不是完全严格的理论叙述，所以没有取题为《信息论基础》。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="信息论" scheme="https://rpche-6626.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>浅谈强化学习的深度学习方法</title>
    <link href="https://rpche-6626.github.io/2025/04/16/DL/RL/"/>
    <id>https://rpche-6626.github.io/2025/04/16/DL/RL/</id>
    <published>2025-04-15T16:00:00.000Z</published>
    <updated>2025-04-15T17:06:53.179Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;p&gt;本文将引入强化学习这一领域，并简要介绍一些基础的理论建模，及其深度学习的解法，例如
Deep Q Learning 和 Policy Gradient</summary>
        
      
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>视频识别与分类</title>
    <link href="https://rpche-6626.github.io/2025/04/12/DL/VDO/"/>
    <id>https://rpche-6626.github.io/2025/04/12/DL/VDO/</id>
    <published>2025-04-11T16:00:00.000Z</published>
    <updated>2025-04-14T16:43:23.697Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文将简要介绍一些从图像识别扩展到视频识别的技术。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>生成模型：自回归模型、变分自编码器与生成对抗网络</title>
    <link href="https://rpche-6626.github.io/2025/04/12/DL/GEN/"/>
    <id>https://rpche-6626.github.io/2025/04/12/DL/GEN/</id>
    <published>2025-04-11T16:00:00.000Z</published>
    <updated>2025-04-14T17:32:21.701Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文将引入一个新的研究分支，即生成模型，并且将简要介绍其中的自回归模型、变分自编码器与生成对抗网络。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>3D 视觉</title>
    <link href="https://rpche-6626.github.io/2025/04/10/DL/3DV/"/>
    <id>https://rpche-6626.github.io/2025/04/10/DL/3DV/</id>
    <published>2025-04-09T16:00:00.000Z</published>
    <updated>2025-04-14T16:53:21.699Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文简要介绍了处理 3D 视觉问题的一些基本技术。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>西罗定理的证明</title>
    <link href="https://rpche-6626.github.io/2025/03/27/%E8%A5%BF%E7%BD%97%E5%AE%9A%E7%90%86%E7%9A%84%E8%AF%81%E6%98%8E/"/>
    <id>https://rpche-6626.github.io/2025/03/27/%E8%A5%BF%E7%BD%97%E5%AE%9A%E7%90%86%E7%9A%84%E8%AF%81%E6%98%8E/</id>
    <published>2025-03-26T16:00:00.000Z</published>
    <updated>2025-04-04T10:35:45.498Z</updated>
    
    
    <summary type="html">&lt;p&gt;感觉中科大那本书的证明写得有烂啊，本文主要参照了华师大版近世代数的证明。&lt;/p&gt;
&lt;p&gt;阅读本文需要一定的群论基础。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="近世代数" scheme="https://rpche-6626.github.io/tags/%E8%BF%91%E4%B8%96%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>物件侦测与语义分割</title>
    <link href="https://rpche-6626.github.io/2025/03/26/DL/OD/"/>
    <id>https://rpche-6626.github.io/2025/03/26/DL/OD/</id>
    <published>2025-03-25T16:00:00.000Z</published>
    <updated>2025-04-14T16:51:19.661Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文简要介绍了用于 Object Detection 的 Region Proposal 以及 R-CNN
。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯方法与逻辑回归</title>
    <link href="https://rpche-6626.github.io/2025/03/24/ML/NB&amp;LR/"/>
    <id>https://rpche-6626.github.io/2025/03/24/ML/NB&amp;LR/</id>
    <published>2025-03-23T16:00:00.000Z</published>
    <updated>2025-03-25T15:40:32.295Z</updated>
    
    
    <summary type="html">&lt;p&gt;本来是想按照课程内容写的，但后来还是决定另起炉灶了。本文将简要的介绍&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="机器学习" scheme="https://rpche-6626.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络的可视化与可解释性</title>
    <link href="https://rpche-6626.github.io/2025/03/21/DL/CNNapp/"/>
    <id>https://rpche-6626.github.io/2025/03/21/DL/CNNapp/</id>
    <published>2025-03-20T16:00:00.000Z</published>
    <updated>2025-03-21T17:02:44.770Z</updated>
    
    
    <summary type="html">&lt;p&gt;CNN 的中间结果可视化技术原来还可以拿来干这些事情。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制</title>
    <link href="https://rpche-6626.github.io/2025/03/18/DL/ATT/"/>
    <id>https://rpche-6626.github.io/2025/03/18/DL/ATT/</id>
    <published>2025-03-17T16:00:00.000Z</published>
    <updated>2025-03-18T16:55:44.792Z</updated>
    
    
    <summary type="html">&lt;p&gt;Attention Is All You Need.&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="https://rpche-6626.github.io/2025/03/18/DL/RNN/"/>
    <id>https://rpche-6626.github.io/2025/03/18/DL/RNN/</id>
    <published>2025-03-17T16:00:00.000Z</published>
    <updated>2025-03-18T12:42:49.304Z</updated>
    
    
    <summary type="html">&lt;p&gt;我本来还以为 RNN 会更厉害一点来着。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>如何训练神经网络</title>
    <link href="https://rpche-6626.github.io/2025/03/11/DL/train/"/>
    <id>https://rpche-6626.github.io/2025/03/11/DL/train/</id>
    <published>2025-03-10T16:00:00.000Z</published>
    <updated>2025-03-18T12:39:04.549Z</updated>
    
    
    <summary type="html">&lt;p&gt;简要介绍了训练神经网络的技术细节，并引入了 Transfer Learning
这一思想。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>卷积神经网络的发展</title>
    <link href="https://rpche-6626.github.io/2025/03/07/DL/CNNdev/"/>
    <id>https://rpche-6626.github.io/2025/03/07/DL/CNNdev/</id>
    <published>2025-03-06T16:00:00.000Z</published>
    <updated>2025-03-07T14:56:51.596Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文梳理了 CNN 模型的发展，简要介绍了 AlexNet 、VGG Net 、Google Net
以及 ResNet 。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="https://rpche-6626.github.io/2025/03/07/DL/CNN/"/>
    <id>https://rpche-6626.github.io/2025/03/07/DL/CNN/</id>
    <published>2025-03-06T16:00:00.000Z</published>
    <updated>2025-04-12T08:17:35.583Z</updated>
    
    
    <summary type="html">&lt;p&gt;这应该是目前最强大的处理图像识别问题的工具。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
    <category term="深度学习" scheme="https://rpche-6626.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>反向传播</title>
    <link href="https://rpche-6626.github.io/2025/03/06/DL/BP/"/>
    <id>https://rpche-6626.github.io/2025/03/06/DL/BP/</id>
    <published>2025-03-05T16:00:00.000Z</published>
    <updated>2025-03-06T15:03:08.909Z</updated>
    
    
    <summary type="html">&lt;p&gt;即 Back Propagation 。
如果你试着实现过神经网络，你就会发现进行梯度下降时手工计算神经网络的损失函数的形式导数是极为繁琐的。所以现在我们要来谈点求导的技术。&lt;/p&gt;</summary>
    
    
    
    <category term="学术" scheme="https://rpche-6626.github.io/categories/%E5%AD%A6%E6%9C%AF/"/>
    
    
  </entry>
  
</feed>

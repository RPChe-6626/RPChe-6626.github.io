<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="OI" />
       
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>如何训练神经网络 |  RPChe_&#39;s Blog</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="RPChe_'s Blog" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-DL/train"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  如何训练神经网络
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/03/11/DL/train/" class="article-date">
  <time datetime="2025-03-10T16:00:00.000Z" itemprop="datePublished">2025-03-11</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">3.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">13 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>简要介绍了训练神经网络的技术细节，并引入了 Transfer Learning 这一思想。</p>
<a id="more"></a>
<script type="math/tex; mode=display">
\newcommand {\bb}{\mathbb}
\newcommand {\la}{\lambda}
\newcommand {\al}{\alpha}
\newcommand {\V}{\operatorname V}</script><h1 id="训练前"><a href="#训练前" class="headerlink" title="训练前"></a>训练前</h1><ul>
<li>本节将会介绍激活函数的选择、对训练数据的处理、参量的初始化以及正则化的选择。</li>
</ul>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul>
<li><p>激活函数是一个非线性函数，其作用是改变网络层的输出，避免网络的表达能力减弱，以区别 Linear Classifier ；另一方面，激活函数也可以视作对神经元的所谓 firing rate 的模拟。</p>
<p>激活函数的选择是多样的，一般有以下几种：</p>
<p><img src="https://cdn.luogu.com.cn/upload/image_hosting/s8i409eh.png" alt=""></p>
<p>我们对以上的几种激活函数做一个简要的评论。</p>
<ol>
<li><p>Sigmoid 。这是最早的选择之一。Sigmoid 有很多坏处，主要是：</p>
<ol>
<li>Sigmoid 在偏离 0 的位置会导致梯度消失，这使得模型迭代缓慢，很难收敛。</li>
<li>Sigmoid 不是 0 centered 。更确切的说，Sigmoid 是恒正的。这同样会给模型的迭代造成问题。具体的，我们考虑 sigmoid 的下一层网络，不妨假设这是一个全连接层。考虑与某一个 hidden unit 有关的所有参数，由于经过了 Sigmoid ，它们的 local gradient 一定全是正的。而与这些参量有关的 upstream gradient 是一个标量，因此这些参量关于损失函数的 gradient 一定是同号的。假设一共有 $d$ 个这样的参量，那么在优化空间 $\bb R^d$ 中我们只能向两个象限移动，而象限总数是 $2^d$ ，这很容易使得优化时要做阶梯状的蠕动，导致迭代缓慢。</li>
<li><code>exp</code> 算得很慢。</li>
</ol>
<p>其实最主要的问题是第一点，其余两点相比之下影响很小。对于第二点，实际上训练的时候我们是会取一个 mini batch ，然后计算它们的梯度的均值的，这实际上会有很大的改善。</p>
</li>
<li><p>tanh 。这其实是对 Sigmoid 的改进。但是它还是会导致梯度消失。</p>
</li>
<li><p>ReLU 。这其实是一个很现代的选择。它的最大优点是解决了梯度消失问题，从而使得模型的收敛速度比 Sigmoid 快了非常多。但 ReLU 仍然不是 0 centered ，而且在负数上没有梯度。这会导致另一个问题，称作 Dead ReLU ，即如果当前的参量远离数据点，使得所有输出全是负的，那梯度就永远不会传播了。<sup><a href="#fn_1" id="reffn_1">1</a></sup>但实践中其实人们一般不用担心这种问题，因为它很少出现。</p>
</li>
<li><p>Leaky ReLU 。这是对 ReLU 的改进，主要目标是避免 dead ReLU 出现。这样做会引入一个超参数，我们也可以选择去学习它，这样就得到了 PReLU 。</p>
</li>
<li><p>ELU 。还是对 ReLU 的改进，可以保证输出是 0 centered 。如果给 ELU 乘一个特定的系数 $\la$ ，并且固定特定的 $\al$ ，就得到了 SELU 。理论上 SELU 具备 Self-Normalizing 性质，即使不用 Batch Normalization 也可以训练深度网络，但是我们无意详谈。<sup><a href="#fn_2" id="reffn_2">2</a></sup></p>
</li>
</ol>
<p>总体来说，激活函数的选择其实没什么大的区别。Justin 建议用 ReLU 就行，还有别用 Sigmoid 和 tanh 。<sup><a href="#fn_11" id="reffn_11">11</a></sup></p>
</li>
</ul>
<blockquote id="fn_1">
<sup>1</sup>. 可能的解决方案是，给输出加上一个小正数。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. 参见 Klambauer et al, Self-Normalizing Neural Network, ICLR 2017 上的 91 页证明。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_11">
<sup>11</sup>. 特别的，激活函数一般都是单调的。一个看法是非单调函数可能会导致信息的损失，这是我们所不希望的。<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a>
</blockquote>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><ul>
<li>一般来说，给定数据点，常见的预处理方式是减去其均值再归一化其方差，或者也可以做 PCA 和 Whitening 。这样搞的好处，我猜测是有助于改善精度问题。比方说考虑用通过原点的超平面划分数据，在减去均值之前，分类结果可能对参量的微小变动很敏感；而减去均值之后，数据点会离原点更近，从而对参量的微小变动不那么敏感。<sup><a href="#fn_3" id="reffn_3">3</a></sup></li>
<li>图像预处理的方案有对每个像素点减去均值，或者对每个 channel 减去均值再归一化方差。 </li>
</ul>
<blockquote id="fn_3">
<sup>3</sup>. 没看出来有什么别的道理，所以不详谈了。<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<h2 id="参量初始化"><a href="#参量初始化" class="headerlink" title="参量初始化"></a>参量初始化</h2><ul>
<li><p>现在考虑如何初始化神经网络中的参数矩阵。具体而言，理想的初始化应该可以保证通畅的梯度传播。一个不错的办法是直接给矩阵填入采样自高斯分布的小随机数，但是对于深层网络来说，这样会出问题。具体的，我们以全连接层为例，考虑计算其输出的方差，得到：</p>
<p><img src="https://cdn.luogu.com.cn/upload/image_hosting/upg81nfu.png" alt=""></p>
<p>假设激活函数是 tanh ，其在 0 附近表现为单位函数。如果 $ D\cdot V[x_i]<1 $ ，那么随着深度的推进，方差会趋于 0 ，每一层的输出也会趋 0 ，导致反向传播的梯度也趋 0 ；如果 $ D\cdot V[x_i]>1 $ ，那么方差会发散，输出会进入 tanh 的 saturated regime ，梯度又会趋 $0$ 。所以我们希望 $D\cdot \V[x_i]=1$ ，这样便使得方差保持定值。<sup><a href="#fn_4" id="reffn_4">4</a></sup></p>
<p>然而如果使用 ReLU 作为激活函数，这样操作还是会导致方差趋于 0 。大致原因是 ReLU 每次都强制一半的输出变成 0 ，所以减小了方差。相应的解决方案则是令 $D\cdot \V[x_i]=2$ 。<sup><a href="#fn_5" id="reffn_5">5</a></sup></p>
<p>如果我们考虑 Residual Block ，会发现以上的操作又失效了。这时的解决方案是，对于 Residual Block 的第一层，采用 MSRA Initialization ；而对于第二层，直接初始化为 0 。<sup><a href="#fn_6" id="reffn_6">6</a></sup><sup><a href="#fn_7" id="reffn_7">7</a></sup></p>
</li>
</ul>
<blockquote id="fn_4">
<sup>4</sup>. 称为 Xavier Initialization 。<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. 称为 MSRA Initialization 。可以想象我们通过这种方式抵消了 ReLU 导致的方差缩减。<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. 阻断了层之间的方差传递，直接令 Residual Path 传递相同的方差。<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_7">
<sup>7</sup>. 如何初始化参数矩阵这个问题其实有很多研究，Justin 列出了其中的一部分。<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a>
</blockquote>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><ul>
<li>正则化的目的是避免过拟合。之前我们已经介绍过了 $L_2$ 初始化，其旨在让模型在优化损失函数之外再做点别的事，比如使得参数分布更均匀。下面我们会介绍一些别的 Regularization 。</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><ul>
<li><p>Dropout 主要用于全连接层。其思想是，每一次由当前层向后更新时，我们都以固定概率（例如 0.5）随机丢弃一些节点，如下图所示：</p>
<p><img src="https://cdn.luogu.com.cn/upload/image_hosting/lhheu4rf.png" alt=""></p>
<p>除开引入随机性从而避免过拟合之外，这样做还有一个好处，就是使得模型在丢弃一部分节点后仍然可以分类。这样做有利于强制不同的节点辨认图片的某些特征，而非整体，从而提高了 Robustness 。<sup><a href="#fn_8" id="reffn_8">8</a></sup>而训练完成以后，为了去除随机性，我们希望对所有情况做平均，而这是很难办到的。一个折衷的方法是，我们考虑每一个神经元受上一层的贡献，发现就是全连接时的贡献再乘上不丢弃的概率，于是我们直接按照这个观察进行计算。<sup><a href="#fn_9" id="reffn_9">9</a></sup></p>
<p>另一方面，我们也可以将 Dropout 看成某种集成学习，即我们同时训练了很多共享参数，但结构不同的模型，最后再做平均。</p>
</li>
</ul>
<blockquote id="fn_8">
<sup>8</sup>. Justin 的 slides 里还有一些很直观的图例。为了控制篇幅，这里没有放。<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_9">
<sup>9</sup>. 这样做是无法计算真实的平均值的，但是实践中的表现已经足够好了。<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a>
</blockquote>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><ul>
<li>Batch Normalization 也可以视作 Regularization。这是因为我们的计算结果和 mini batch 的取法是有关的，自然引入了随机性。</li>
<li>特别的，Dropout 一般只用于全连接层，而后来的 CNN 基本很少用全连接层了。这时采取的 regularization 一般就是 Batch Normalization 。</li>
</ul>
<h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><ul>
<li>我们也可以考虑对训练集做随机变化来增强数据集，也就引入了随机性。常用的方式包括随机裁剪、改变尺寸，随机去掉图片的一部分（Cutout）、或者将不同的图片混在一块（Mixup）。</li>
</ul>
<h3 id="其它方式"><a href="#其它方式" class="headerlink" title="其它方式"></a>其它方式</h3><ul>
<li>Fractional Max Pooling（做随机大小的 pooling）、Stochastic Depth（随机跳过一些 Layer 或者 Block）、Drop Connect（随机去掉全连接层往后贡献的转移边），之类的，选择很多。推荐的方式包括 Batch Normalization 、Data Augmentation 、Cutout 和 Mixup 。</li>
</ul>
<h1 id="训练中"><a href="#训练中" class="headerlink" title="训练中"></a>训练中</h1><h2 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h2><ul>
<li>一般的选择是将学习率设置为常数，在很多时候这都是一个很好的选择。另外也可以考虑设置变化的学习率，比方说每过 30 个 epoch 就将学习率减小到原先的 0.1 ，称作 Step learning rate 。<sup><a href="#fn_10" id="reffn_10">10</a></sup>另一个常见的选择是 Cosine learning rate ，按照：<script type="math/tex; mode=display">
\al_t=\frac12\al_0 (1+\cos (t\pi/T))</script>这是 CV 领域较为传统的选择。而 NLP 方向则一般会倾向于线性的学习率：<script type="math/tex; mode=display">
\al_t=\al_0 (1-t/T)</script>另外还有一些比较小众的选择，例如 Inverse root learning rate ：<sup><a href="#fn_12" id="reffn_12">12</a></sup><sup><a href="#fn_15" id="reffn_15">15</a></sup><script type="math/tex; mode=display">
\al_t=\al_0/\sqrt t</script></li>
</ul>
<blockquote id="fn_10">
<sup>10</sup>. ResNet 的训练就用到了这样的技巧。epoch 指固定次数的迭代，比方说 300 次。<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_12">
<sup>12</sup>. 被用于 Vaswanl et al, Attention is all you need, NeurIPS 2017 。<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_15">
<sup>15</sup>. 一般来说，我们希望变动量与参数大小（$L_2$ 范数之类的）的比值在 0.001 左右。<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a>
</blockquote>
<h2 id="如何看待学习曲线"><a href="#如何看待学习曲线" class="headerlink" title="如何看待学习曲线"></a>如何看待学习曲线</h2><ul>
<li>我们最关注的学习曲线一般是损失函数曲线（作散点图并求均值）以及训练集和验证集上的正确率。我们一般会考虑在损失函数不再有明显下降时减小学习率。而如果在训练集上正确率提升时验证正确率下降，说明产生过拟合，我们可能会考虑选择验证正确率最高处的参量，或者调整正则化、引入更多数据。如果两个正确率离得很近，说明模型还是欠拟合，我们可能会考虑急需训练，或者增大参数量。总体来说，两个正确率都应该增长，并带有一定差距。</li>
</ul>
<h2 id="如何选择超参数"><a href="#如何选择超参数" class="headerlink" title="如何选择超参数"></a>如何选择超参数</h2><ul>
<li>一般的选择是作 Grid Search ，即为每个超参数选择一些值<sup><a href="#fn_13" id="reffn_13">13</a></sup>，然后尝试所有组合，选出最好的一个。这样做的坏处是，假设我们做了 $n$ 次尝试，那么单一超参数上的采样是 $\log n$ 级别的。一个相对更好的方式是，我们为每个超参数选定一个区间，然后随机尝试 $n$ 次，每个超参数均独立随机采样。这样就可以让单一超参数的采样增大到 $n$ 级别。</li>
<li><p>理论上我们也可以对超参数做优化，例如做 Gradient Descent 之类的，但是这样的计算开销太大了。这方面其实也有一些研究，但我们无意深入。</p>
</li>
<li><p>最后我们将给出一个选择超参数的范式。<sup><a href="#fn_14" id="reffn_14">14</a></sup></p>
<ol>
<li>关闭正则化，使用采样自小方差高斯分布的参量进行单次计算，查看输出是否符合预期，从而避免网路写挂。</li>
<li>关闭正则化，使用小训练集训练模型，并过拟合。其旨在通过快速调整找到较好的网络结构、学习率以及参量初始化的设置。</li>
<li>使用上一步的网络结构，打开正则化，使用所有训练数据，找到合适的学习率，使得前 100 步迭代内损失函数有显著下降。</li>
<li>在上一步的基础上找到大致的超参数区间，然后训练 1 到 5 个 epoch 。</li>
<li>调整参数区间，延长训练。</li>
<li>查看学习曲线。</li>
<li>回到第五步。</li>
</ol>
</li>
</ul>
<blockquote id="fn_13">
<sup>13</sup>. 一般是 $\log$ linear 的。<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_14">
<sup>14</sup>. 特别的，我们可以使用 Cross Validation 来并行训练模型。数据可视化则推荐 Tensor Board 。<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a>
</blockquote>
<h1 id="训练后"><a href="#训练后" class="headerlink" title="训练后"></a>训练后</h1><ul>
<li>我们可能会希望对训练后的模型做一些调整或整合，本节将简要介绍。</li>
</ul>
<h2 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h2><ul>
<li>训练多个模型，然后将它们的概率分布取均值，或者使用投票法之类的。</li>
<li>或者也可以截取模型训练过程中的快照来集成。为此，我们可能会考虑使用周期性的学习率，并在低点保存模型。</li>
<li>或者取训练过程中的参量的指数均值之类的。<sup><a href="#fn_16" id="reffn_16">16</a></sup></li>
</ul>
<blockquote id="fn_16">
<sup>16</sup>. 类似于 <code>x_test = 0.995 * x_test + 0.005 * x</code> 。其中 <code>x</code> 是参量。称作 Polyak Averaging 。<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a>
</blockquote>
<h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><ul>
<li><p>迁移学习是一个相当重要的思想，即将大数据集上已有的模型迁移到小数据集上。一个典型的例子是，使用 ImageNet 上训练完成的模型，例如 VGG 16 ，去除最后的全连接层，迁移到 Caltech101 上，并冻结所有参数，就可以取得比单独训练的模型好得多的成果。<sup><a href="#fn_17" id="reffn_17">17</a></sup>而这样的现象是相当广泛的。<sup><a href="#fn_18" id="reffn_18">18</a></sup></p>
<p>一般的，我们可以考虑将大数据集上训练完成的模型的参数冻结、去除最后的全连接层，用作图像特征提取器。然后我们再使用 SVM 来划分不同的类别。这样的方式一般适用于与大数据集具备较高相似度的小数据集<sup><a href="#fn_19" id="reffn_19">19</a></sup>，例如 ImageNet 与 Caltech101 。</p>
<p>另一个方式则被称作 Fine-Tuning ，即，我们去除训练完成的模型的最后的全连接层，然后添加适用于新数据集的全连接层。具体的，我们可以考虑先用 feature extractor 训练一个 Linear Classifier ，然后将其连接到模型上形成最后的全连接层，再做整体的训练。特别的，使用 Fine-Tuning 时一般需要减小学习率，有时我们也会考虑冻结模型的浅层参数以节省计算资源。Fine-Tuning 一般要求新数据集不能太小，同时冻结的层数也是与数据集的相似度有关的。</p>
</li>
</ul>
<blockquote id="fn_17">
<sup>17</sup>. 当然，一般在 ImageNet 上表现更好的模型迁移以后也会更好。需要指出的是，迁移是一个好的选择，但不一定是必要的。在新数据集上单独训练可能也能取得不错的成果，但是一般而言耗时会更长。另一方面，预训练非常耗时，相比之下扩大数据集可能是更有效的选择。<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_18">
<sup>18</sup>. 可以广泛的迁移到其它模型、其它问题当中。<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_19">
<sup>19</sup>. 如果相似度不高，可以考虑把中间层的结果拿来当特征向量跑 Linear Classifier 试试。<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a>
</blockquote>
<h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><ul>
<li>先别管这个。哥们也没几张卡。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://rpche-6626.github.io/2025/03/11/DL/train/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2025/03/18/DL/RNN/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            循环神经网络
          
        </div>
      </a>
    
    
      <a href="/2025/03/07/DL/devCNN/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">卷积神经网络的发展</div>
      </a>
    
  </nav>

  
   
<div class="gitalk" id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.staticfile.org/gitalk/1.7.2/gitalk.min.css">


<script src="https://cdn.staticfile.org/gitalk/1.7.2/gitalk.min.js"></script>


<script src="https://cdn.staticfile.org/blueimp-md5/2.19.0/js/md5.min.js"></script>

<script type="text/javascript">
  var gitalk = new Gitalk({
    clientID: 'Ov23liT3QoEfJXeCUfju',
    clientSecret: '5f4e2837d266117fa40abce36a021c80c9f72a30',
    repo: 'Gitalk-Blog-Comments',
    owner: 'RPChe-6626',
    admin: ['RPChe-6626'],
    // id: location.pathname,      // Ensure uniqueness and length less than 50
    id: md5(location.pathname),
    distractionFreeMode: false,  // Facebook-like distraction free mode
    pagerDirection: 'last'
  })

  gitalk.render('gitalk-container')
</script>

  
    
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2025
        <i class="ri-heart-fill heart_icon"></i> RPChe_
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="RPChe_&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>